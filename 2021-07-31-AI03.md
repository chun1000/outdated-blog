---
layout: post
title: 인공지능03 - 머신 러닝
categories: [Today i learn]
tags: [TIL, AI]
description: 머신 러닝에 대해서.
---

### 1. K-means

---

- 1NN은 가장 가까운 것.
- KNN은 다수결.

### 2. 정규화

---

![image-20210731114525022](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114525022.png)

### 3. 결정 트리

---

![image-20210731114610421](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114610421.png)

- 엔트로피로 선택한다.
  ![image-20210731114658689](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114658689.png)
- 정보의 수준이 높으면 엔트로피가 낮아진다.
- 주사위를 던질 때 짝수가 나올지 홀수가 나올지 확률이 1/2라면, 정보가 없는 것이다. ![image-20210731114753405](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114753405.png) 즉 1이 나온다.
- 던지면 무조건 짝수가 나오면 정보가 많은 것이다. 엔트로피는 0이다.![image-20210731114832130](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114832130.png)

![image-20210731114902943](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114902943.png)

![image-20210731114911205](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114911205.png)

![image-20210731114919719](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114919719.png)

- Information gain.

---

![image-20210731114952571](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114952571.png)

![image-20210731114943855](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731114943855.png)

![image-20210731115210590](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731115210590.png)

![image-20210731115240819](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731115240819.png)



### 4. Naive Bayes

---

![image-20210731121526324](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731121526324.png)

- 조건부 독립이라고 가정하면 계산하기 쉽다.

![image-20210731122214670](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731122214670.png)

### 5. Text 분류

---

- Terim-Frequency(TF): 얼마나 자주 나타나는가
- Inverse-document frequency(IDF): term이 더 적은 문서에서 발견되면 가중치를 더한다.
- Word embeddings: 단어를 dimension으로 표시한다. 앞 뒤로 열개 정도의 상관도를 계산해서 보는 등의 접근 방식을 생각할 수 있다.

![image-20210731122455741](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210731122455741.png)

### 6. 클러스트링

---

- EM은 centroid에 속할 확률 maximize하고 다시 계산하고.
- Hierarchical Clustering: 계층 관계에 따라 클러스터를 가까운것과 합치는 방식.

