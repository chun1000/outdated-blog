---
layout: post
title: 기계학습07 - Bias와 Variance
categories: [Today i learn]
tags: [TIL, AI]
description: 바이어스와 베어리언스.
---

### 1. Debugging a learning algorithm

---

![image-20210729215759758](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729215759758.png)

- 트레이닝과 테스트 셋을 exclusive하게 나누어야한다.

![image-20210729220337533](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729220337533.png)

- 학습할 때 쓰는게 아니기 때문에 regularization term이 붙지 않는다.

![image-20210729220404334](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729220404334.png)

- logistic regression은 저런식으로 테스트한다. 

### 2. Cross Validation

---

![image-20210729220442137](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729220442137.png)

- cross validation set을 트레이닝에서 빼서 모델을 만들기 위해 사용. 모델의 성능을 평가하기 위해 사용한다.

![image-20210729220701194](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729220701194.png)

- fold를 접어서 validation set을 여러개 만들수도 있다.(K-fold)

---

![image-20210729221402035](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729221402035.png)

- 트레이닝 에러는 오버피팅이면 작아지지만, cv는 갑자기 나빠진다.(training DB에 맞춰져서 바보가 됐다.)
- 언더피팅이면 그냥 다 나쁘다.

![image-20210729221556106](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729221556106.png)

- 람다값이 크면 언더피팅, 작으면 오버피팅.

### 3. Learning curves

---

![image-20210729221929507](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729221929507.png)

- 데이터를 많이 모으면 무조건 트레이닝 셋 오차는 증가한다.(데이터가 없으면 오차를 줄이기 쉽다.)
- cv는 반대로 점점 줄어든다.

---

![image-20210729222011970](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729222011970.png)

- 언더 피팅 되어있으면 cv가 떨어지긴 하는데, training 에러가 날카롭게 증가하며, 위로 붕떠서 둘 사이의 간격이 좁아진다.

![image-20210729222044535](https://raw.githubusercontent.com/chunyunseo/ImageRepo/image/img/image-20210729222044535.png)

- 오버피팅되면 둘 사이의 갭이 엄청 커진다.

### 4. What to do

---

- 트레이닝 데이터를 모은다 -> high variance 해결.
- 피쳐를 줄인다 -> high variance 해결.
- 피쳐를 늘린다  -> high bias 해결.
- polynomial features를 사용한다 -> high bias 해결.
- 람다를 줄인다 -> high bias 해결.
- 람다를 늘린다 -> high variance 해결.

